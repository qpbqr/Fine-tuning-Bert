{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "321831c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import gc\n",
    "import shutil\n",
    "\n",
    "device = torch.device('cpu')\n",
    "SIMILARITY_THRESHOLD = 0.9\n",
    "TARGET_PER_TICKER = 3000\n",
    "POSITIVE_PER_TICKER = 1500\n",
    "NEGATIVE_PER_TICKER = 1500\n",
    "CHUNKSIZE = 50000\n",
    "MAX_LEN = 128\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "EMBEDDING_BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90ca176",
   "metadata": {},
   "source": [
    "We want to use contextual-embedding to deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae04697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading tokenizer...\n",
      "loading model and applying dynamic quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pengb\\Desktop\\thesis_project\\.conda\\Lib\\site-packages\\huggingface_hub\\file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model optimized (int8 quantized) ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"loading tokenizer...\")\n",
    "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"loading model and applying dynamic quantization...\")\n",
    "base_model = BertModel.from_pretrained(MODEL_NAME)\n",
    "bert_model = torch.quantization.quantize_dynamic(\n",
    "    base_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "bert_model.eval()\n",
    "\n",
    "print(\"Model optimized (int8 quantized) ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ade13c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_embedding(texts, batch_size = 64):\n",
    "    all_embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "\n",
    "        #TOKENIZE\n",
    "        encoded = tokenizer.batch_encode_plus(\n",
    "            batch_texts,\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            padding = 'max_length',\n",
    "            truncation = True,\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt',\n",
    "        )\n",
    "        input_ids = encoded['input_ids']\n",
    "        attention_mask = encoded['attention_mask']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min = 1e-9)\n",
    "            embeddings = (sum_embeddings / sum_mask).numpy()\n",
    "            norms = np.linalg.norm(embeddings, axis = 1, keepdims = True)\n",
    "            embeddings = embeddings / (norms + 1e-9)\n",
    "\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96c0cece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_ticker_data(df_ticker):\n",
    "    if len(df_ticker) == 0:\n",
    "        return df_ticker\n",
    "\n",
    "    # hard-coded deduplication\n",
    "    # if the headline is the same, keep the one with the largest abs(event_sentiment_score)\n",
    "    original_len = len(df_ticker)\n",
    "    df_ticker['abs_score'] = df_ticker['event_sentiment_score'].abs()\n",
    "    df_ticker = df_ticker.sort_values(by = 'abs_score', ascending = False)\n",
    "    df_ticker = df_ticker.drop_duplicates(subset = ['headline'], keep = 'first')\n",
    "    df_ticker = df_ticker.drop(columns = ['abs_score']).reset_index(drop = True)\n",
    "\n",
    "    if len(df_ticker) < 2:\n",
    "        return df_ticker\n",
    "\n",
    "    # contextual-embedding deduplication\n",
    "    texts = df_ticker['headline'].astype(str).str.strip().tolist()\n",
    "    embeddings = get_bert_embedding(texts, batch_size = EMBEDDING_BATCH_SIZE)\n",
    "    sim_matrix = np.dot(embeddings, embeddings.T)\n",
    "    high_sim_indices = np.where(np.triu(sim_matrix, k = 1) > SIMILARITY_THRESHOLD)\n",
    "\n",
    "    to_remove = set()\n",
    "    scores = df_ticker['event_sentiment_score'].values\n",
    "\n",
    "    for idx1, idx2 in zip(high_sim_indices[0], high_sim_indices[1]):\n",
    "        if idx1 in to_remove or idx2 in to_remove:\n",
    "            continue\n",
    "\n",
    "        if abs(scores[idx1]) >= abs(scores[idx2]):\n",
    "            to_remove.add(idx2)\n",
    "        else:\n",
    "            to_remove.add(idx1)\n",
    "\n",
    "    keep_indices = [i for i in range(len(df_ticker)) if i not in to_remove]\n",
    "    df_final = df_ticker.iloc[keep_indices].reset_index(drop = True)\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec97bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_samples(df_dedup):\n",
    "    if len(df_dedup) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    if 'label' not in df_dedup.columns:\n",
    "        df_dedup['label'] = df_dedup['event_sentiment_score'].apply(\n",
    "            lambda x: 1 if x > 0.2 else (0 if x < 0 else 2)\n",
    "        )\n",
    "    \n",
    "    df_positive = df_dedup[df_dedup['label'] == 1]\n",
    "    df_negative = df_dedup[df_dedup['label'] == 0]\n",
    "\n",
    "    # sampling\n",
    "    n_pos = min(len(df_positive), POSITIVE_PER_TICKER)\n",
    "    n_neg = min(len(df_negative), NEGATIVE_PER_TICKER)\n",
    "\n",
    "    if n_pos > 0:\n",
    "        pos_sampled = df_positive.sample(n = n_pos, random_state = 42)\n",
    "    else:\n",
    "        pos_sampled = pd.DataFrame()\n",
    "    \n",
    "    if n_neg > 0:\n",
    "        neg_sampled = df_negative.sample(n = n_neg, random_state = 42)\n",
    "    else:\n",
    "        neg_sampled = pd.DataFrame()\n",
    "    \n",
    "    df_final = pd.concat([pos_sampled, neg_sampled], ignore_index = True)\n",
    "    \n",
    "    return df_final.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80467e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_pipeline_stratified(input_path, output_path, temp_base_dir=\"./temp_pipeline\"):\n",
    "    # parameters\n",
    "    RAW_POS_LIMIT = int(POSITIVE_PER_TICKER * 8)\n",
    "    RAW_NEG_LIMIT = int(NEGATIVE_PER_TICKER * 8)\n",
    "    \n",
    "    print(f\"Stratified Collection Targets per Ticker:\")\n",
    "    print(f\"  - Raw Positive needed: {RAW_POS_LIMIT}\")\n",
    "    print(f\"  - Raw Negative needed: {RAW_NEG_LIMIT}\")\n",
    "    print(f\"  - Neutral: Ignored (to save space)\")\n",
    "    \n",
    "    # directory preparation\n",
    "    temp_ticker_dir = os.path.join(temp_base_dir, \"raw_tickers\")\n",
    "    if os.path.exists(temp_base_dir):\n",
    "        shutil.rmtree(temp_base_dir)\n",
    "    os.makedirs(temp_ticker_dir, exist_ok=True)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "    \n",
    "    # Step 1: Scanning & Stratified Sampling (Pos/Neg)\n",
    "    print(\"=\"*60)\n",
    "    print(\"Step 1: Scanning & Stratified Sampling (Pos/Neg)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # counter structure: counts[ticker] = {'pos': 0, 'neg': 0}\n",
    "    ticker_counts = defaultdict(lambda: {'pos': 0, 'neg': 0})\n",
    "    # record completed tickers (both pos and neg are full)\n",
    "    ticker_completed = set()\n",
    "    \n",
    "    seen_hashes = set() \n",
    "    \n",
    "    chunk_iter = pd.read_csv(input_path, chunksize=CHUNKSIZE, low_memory=False)\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunk_iter, desc=\"Reading Chunks\")):\n",
    "        # 1. basic cleaning\n",
    "        chunk = chunk.dropna(subset=['headline', 'event_sentiment_score', 'ticker'])\n",
    "        chunk['headline'] = chunk['headline'].astype(str).str.strip()\n",
    "        chunk = chunk[chunk['headline'] != '']\n",
    "        \n",
    "        # 2. fast deduplication (String Hash)\n",
    "        current_hashes = chunk['headline'].apply(hash)\n",
    "        is_new = ~current_hashes.isin(seen_hashes)\n",
    "        seen_hashes.update(current_hashes[is_new])\n",
    "        chunk = chunk[is_new]\n",
    "        \n",
    "        if len(chunk) == 0: continue\n",
    "            \n",
    "        # 3. exclude tickers that are already full\n",
    "        chunk = chunk[~chunk['ticker'].isin(ticker_completed)]\n",
    "        if len(chunk) == 0: continue\n",
    "            \n",
    "        # 4. label\n",
    "        # 1: Positive, 0: Negative, 2: Neutral\n",
    "        # note: here we directly filter out neutral data (Label 2), because we only need positive and negative\n",
    "        labels = chunk['event_sentiment_score'].apply(\n",
    "            lambda x: 1 if x > 0.2 else (0 if x < 0 else 2)\n",
    "        )\n",
    "        chunk = chunk.assign(label=labels)\n",
    "        \n",
    "        # 6. only keep Positive (1) and Negative (0)\n",
    "        chunk = chunk[chunk['label'].isin([0, 1])]\n",
    "        if len(chunk) == 0: continue\n",
    "\n",
    "        # 7. distribution logic\n",
    "        for ticker, group in chunk.groupby('ticker'):\n",
    "            if ticker in ticker_completed: continue\n",
    "            \n",
    "            new_pos = group[group['label'] == 1]\n",
    "            new_neg = group[group['label'] == 0]\n",
    "            \n",
    "            current_counts = ticker_counts[ticker]\n",
    "            \n",
    "            # calculate how many more we need\n",
    "            needed_pos = max(0, RAW_POS_LIMIT - current_counts['pos'])\n",
    "            needed_neg = max(0, RAW_NEG_LIMIT - current_counts['neg'])\n",
    "            \n",
    "            to_save_list = []\n",
    "            \n",
    "            if needed_pos > 0 and len(new_pos) > 0:\n",
    "                take_pos = new_pos.head(needed_pos)\n",
    "                to_save_list.append(take_pos)\n",
    "                current_counts['pos'] += len(take_pos)\n",
    "            \n",
    "            if needed_neg > 0 and len(new_neg) > 0:\n",
    "                take_neg = new_neg.head(needed_neg)\n",
    "                to_save_list.append(take_neg)\n",
    "                current_counts['neg'] += len(take_neg)\n",
    "            \n",
    "            # if there is data to save\n",
    "            if to_save_list:\n",
    "                to_save = pd.concat(to_save_list)\n",
    "                save_path = os.path.join(temp_ticker_dir, f\"{ticker}.csv\")\n",
    "                header = not os.path.exists(save_path)\n",
    "                to_save.to_csv(save_path, mode='a', header=header, index=False)\n",
    "            \n",
    "            # check if the ticker is full\n",
    "            if current_counts['pos'] >= RAW_POS_LIMIT and current_counts['neg'] >= RAW_NEG_LIMIT:\n",
    "                ticker_completed.add(ticker)\n",
    "                \n",
    "        if i % 10 == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    print(f\"\\nStep 1 Completed. Collected data for {len(ticker_counts)} tickers.\")\n",
    "    \n",
    "    # === Step 2: Processing per ticker===\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Step 2: Processing tickers (Deduplication + Balancing)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    tickers_to_process = sorted(list(ticker_counts.keys()))\n",
    "    write_header = True\n",
    "    processed_buffer = []\n",
    "    stats_list = []\n",
    "    \n",
    "    for ticker in tqdm(tickers_to_process, desc=\"Processing Tickers\"):\n",
    "        file_path = os.path.join(temp_ticker_dir, f\"{ticker}.csv\")\n",
    "        if not os.path.exists(file_path): continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            \n",
    "            # 1. deduplication\n",
    "            df_dedup = deduplicate_ticker_data(df)\n",
    "            \n",
    "            # 2. sampling (ensure 1.5k/1.5k)\n",
    "            df_final = balance_samples(df_dedup)\n",
    "            \n",
    "            # statistics\n",
    "            stats_list.append({\n",
    "                'ticker': ticker,\n",
    "                'raw_pos_collected': len(df[df['label']==1]),\n",
    "                'raw_neg_collected': len(df[df['label']==0]),\n",
    "                'final_pos': len(df_final[df_final['label']==1]),\n",
    "                'final_neg': len(df_final[df_final['label']==0]),\n",
    "            })\n",
    "            \n",
    "            if len(df_final) > 0:\n",
    "                processed_buffer.append(df_final)\n",
    "            \n",
    "            if len(processed_buffer) >= 5:\n",
    "                batch_df = pd.concat(processed_buffer, ignore_index=True)\n",
    "                batch_df = batch_df[['ticker', 'headline', 'event_sentiment_score', 'label']]\n",
    "                batch_df.to_csv(output_path, mode='a', header=write_header, index=False)\n",
    "                write_header = False \n",
    "                processed_buffer = [] \n",
    "                gc.collect()\n",
    "            \n",
    "            os.remove(file_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if len(processed_buffer) > 0:\n",
    "        batch_df = pd.concat(processed_buffer, ignore_index=True)\n",
    "        batch_df = batch_df[['ticker', 'headline', 'event_sentiment_score', 'label']]\n",
    "        batch_df.to_csv(output_path, mode='a', header=write_header, index=False)\n",
    "    \n",
    "    # save statistics\n",
    "    pd.DataFrame(stats_list).to_csv(output_path.replace('.csv', '_stats.csv'), index=False)\n",
    "    shutil.rmtree(temp_base_dir)\n",
    "    print(f\"\\nDone! Final data saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "507049e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified Collection Targets per Ticker:\n",
      "  - Raw Positive needed: 12000\n",
      "  - Raw Negative needed: 12000\n",
      "  - Neutral: Ignored (to save space)\n",
      "============================================================\n",
      "Step 1: Scanning & Stratified Sampling (Pos/Neg)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading Chunks: 2132it [21:12,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1 Completed. Collected data for 499 tickers.\n",
      "\n",
      "============================================================\n",
      "Step 2: Processing tickers (Deduplication + Balancing)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Tickers: 100%|██████████| 499/499 [1:44:09<00:00, 12.53s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done! Final data saved to: ../data/processed/balanced_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_path = \"../data/raw/raw_data.csv\"\n",
    "output_path = \"../data/processed/balanced_data.csv\"\n",
    "\n",
    "process_data_pipeline_stratified(input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
